{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dsp import LM\n",
    "from dspy.datasets import HotPotQA\n",
    "import dspy\n",
    "from dsp.utils import deduplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLMClient(LM):\n",
    "    def __init__(self, model, api_key):\n",
    "        self.model = model\n",
    "        self.api_key = api_key\n",
    "        self.base_url = \"https://proxy.tune.app/chat/completions\"\n",
    "        self.history = []\n",
    "        self.kwargs = {}  # Initialize kwargs here\n",
    "\n",
    "    def basic_request(self, prompt: str, **kwargs):\n",
    "        headers = {\n",
    "            \"Authorization\": f\"{self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "        # Store kwargs for later access (as expected by the ChainOfThought module)\n",
    "        self.kwargs.update(kwargs)\n",
    "\n",
    "        # Define the request payload, including necessary parameters\n",
    "        data = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are TuneStudio, answer the question based on the context given to you.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": kwargs.get(\"temperature\", 0.9),\n",
    "            \"max_tokens\": kwargs.get(\"max_tokens\", 100),\n",
    "            \"frequency_penalty\": kwargs.get(\"frequency_penalty\", 0.2),\n",
    "            \"stream\": kwargs.get(\"stream\", False)\n",
    "        }\n",
    "\n",
    "        # Make the POST request\n",
    "        response = requests.post(self.base_url, headers=headers, json=data)\n",
    "        response_data = response.json()\n",
    "\n",
    "        # Save the prompt and response history\n",
    "        self.history.append({\"prompt\": prompt, \"response\": response_data, \"kwargs\": kwargs})\n",
    "\n",
    "        # Return the raw response data for further processing\n",
    "        return response_data\n",
    "\n",
    "    def __call__(self, prompt, **kwargs):\n",
    "        response = self.basic_request(prompt, **kwargs)\n",
    "        \n",
    "        # Extract the generated text from the correct field in the response structure\n",
    "        try:\n",
    "            completions = [result[\"message\"][\"content\"] for result in response[\"choices\"]]\n",
    "        except KeyError:\n",
    "            raise ValueError(f\"Unexpected response structure: {response}\")\n",
    "        \n",
    "        return completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_lm = CustomLMClient(model='qwen/qwen-2.5-72b', api_key='YOUR_API_REQUEST')\n",
    "colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')\n",
    "dspy.settings.configure(lm=custom_lm, rm=colbertv2_wiki17_abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aryankargwal/miniconda3/lib/python3.12/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset.\n",
    "dataset = HotPotQA(train_seed=1, train_size=20, eval_seed=2023, dev_size=50, test_size=0)\n",
    "\n",
    "# Tell DSPy that the 'question' field is the input. Any other fields are labels and/or metadata.\n",
    "trainset = [x.with_inputs('question') for x in dataset.train]\n",
    "devset = [x.with_inputs('question') for x in dataset.dev]\n",
    "\n",
    "len(trainset), len(devset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplifiedBaleen(dspy.Module):\n",
    "    def __init__(self, lm_client, passages_per_hop=3, max_hops=1):\n",
    "        super().__init__()\n",
    "        self.retrieve = dspy.Retrieve(k=passages_per_hop)\n",
    "        self.max_hops = max_hops\n",
    "        self.lm_client = lm_client  # Store the LM client instance\n",
    "\n",
    "    def generate_query(self, context: list[str], question, **kwargs) -> str:\n",
    "        # Create a search query based on the question and context\n",
    "        query = f\"{question} Context: {' '.join(context)}\"\n",
    "        return query\n",
    "\n",
    "    def generate_answer(self, context: list[str], question, **kwargs) -> str:\n",
    "        # Create a prompt that combines context and question\n",
    "        context_str = \" \".join(context)\n",
    "        prompt = f\"Given the following information: {context_str} \\n\\nAnswer the question: {question}\"\n",
    "        \n",
    "        # Now pass this prompt to the generative model\n",
    "        response = self.lm_client(prompt, **kwargs)  # Use your CustomLMClient instance\n",
    "        answer = response[0]  # Get the first response\n",
    "\n",
    "        return answer\n",
    "\n",
    "    def forward(self, question, **kwargs):\n",
    "        context = []\n",
    "\n",
    "        for _ in range(self.max_hops):\n",
    "            query = self.generate_query(context=context, question=question, **kwargs)\n",
    "            passages = self.retrieve(query).passages\n",
    "            context = deduplicate(context + passages)\n",
    "\n",
    "        answer = self.generate_answer(context=context, question=question, **kwargs)\n",
    "        return dspy.Prediction(context=context, answer=answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What position on the Billboard Top 100 did Alison Moyet's late summer hit achieve?\n",
      "Predicted Answer: Alison Moyet's late summer hit, \"Love Resurrection,\" achieved a position of number 82 on the Billboard Hot 100 in August 1985.\n",
      "Retrieved Contexts (truncated): ['Love Resurrection | \"Love Resurrection\" is a pop song written by English singer-songwriter Alison Moyet and producers Jolley & Swain for Moyet\\'s debut studio album \"Alf\" (1984). Released as the album\\'...', 'All Cried Out (Alison Moyet song) | \"All Cried Out\" is a song by English singer-songwriter Alison Moyet. It was written by Moyet and producers Jolley & Swain for her debut studio album \"Alf\" (1984). R...', 'When I Was Your Girl | \"When I Was Your Girl\" is a song by English singer-songwriter Alison Moyet, released as the first single from her eighth studio album, \"The Minutes\" (2013), which debuted at num...']\n"
     ]
    }
   ],
   "source": [
    "# Initialize and pass kwargs correctly\n",
    "temperature = 0.9  # or any other value\n",
    "max_tokens = 100\n",
    "\n",
    "# Ask any question you like to this simple RAG program.\n",
    "my_question = \"What position on the Billboard Top 100 did Alison Moyet's late summer hit achieve?\"\n",
    "\n",
    "# Get the prediction. This contains `pred.context` and `pred.answer`.\n",
    "uncompiled_baleen = SimplifiedBaleen(lm_client=custom_lm)  # uncompiled (i.e., zero-shot) program\n",
    "\n",
    "# Pass in temperature and other parameters\n",
    "pred = uncompiled_baleen(my_question, temperature=temperature, max_tokens=max_tokens)\n",
    "\n",
    "# Print the contexts and the answer.\n",
    "print(f\"Question: {my_question}\")\n",
    "print(f\"Predicted Answer: {pred.answer}\")\n",
    "print(f\"Retrieved Contexts (truncated): {[c[:200] + '...' for c in pred.context]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
